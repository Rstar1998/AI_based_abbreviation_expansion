{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1651976,"sourceType":"datasetVersion","datasetId":965195}],"dockerImageVersionId":30066,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Abbreviation Disambiguation in Medical Texts - Data Modeling\n\nThis Notebook is in continuation of the notebook- 'Step 2- Data Preprocessing' and lists down:\n\n1. Modeling Preprocessed data using: GridSearchCV on Logistic Regression, SVM and XG Boost.\n2. Testing the models using Test set.\n3. Comparing the models and identifying the Next Steps","metadata":{}},{"cell_type":"markdown","source":"## Step# 1: Loading Dataset","metadata":{}},{"cell_type":"code","source":"#Importing the Required Python Packages\nimport shutil\nimport string\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\nimport ast\nfrom sklearn import utils\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport spacy\n# import xgboost as xgb\n# from xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestRegressor\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:20:34.817100Z","iopub.execute_input":"2024-12-04T07:20:34.817431Z","iopub.status.idle":"2024-12-04T07:20:37.775566Z","shell.execute_reply.started":"2024-12-04T07:20:34.817358Z","shell.execute_reply":"2024-12-04T07:20:37.774905Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lets load the default english model of spacy\n# !python -m spacy download en_core_web_sm\n\nnlp = spacy.load('en_core_web_sm')\n\ndatapath = \"../input/medal-emnlp/pretrain_subset\"\n# datapath = \"archive/pretrain_subset\"\n\n# Lets load the train dataset.\n\ntrain = pd.read_csv(datapath + '/train.csv')\n# train = train[:100000]\n\n# Lets load validation and test datasets as well\nvalid = pd.read_csv(f'{datapath}/valid.csv')\n# test = pd.read_csv(f'{datapath}/test.csv')\n# valid = valid[:10000]\n# test = test[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:20:37.777381Z","iopub.execute_input":"2024-12-04T07:20:37.777634Z","iopub.status.idle":"2024-12-04T07:22:09.092125Z","shell.execute_reply.started":"2024-12-04T07:20:37.777609Z","shell.execute_reply":"2024-12-04T07:22:09.091388Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lets create a function to create a new feature 'ABV' from dataset\ndef createFeature(df):    \n    return [x.split(' ')[y] for x,y in zip(df['TEXT'], df['LOCATION'])]\n\ntrain['ABV'] = createFeature(train)\nvalid['ABV'] = createFeature(valid)\n# test['ABV'] = createFeature(test)\n\ngrouped = train.groupby(by=['ABV', 'LABEL'], as_index = False, sort = False).count()\ngrouped = grouped.sort_values(by='TEXT', ascending = False)\n\n# topAbv = grouped['ABV'][:20]\n\n# train = train[train['ABV'].isin(topAbv)]\n# valid = valid[valid['ABV'].isin(topAbv)]\n# test = test[test['ABV'].isin(topAbv)]\n\n# Lets create a function to remove all the Punctuations from Text\ndef removePunctuation(df):\n    return [t.translate(str.maketrans('','',string.punctuation)) for t in df['TEXT']]\n\n# Lets create a function to Tokenize the Text column of dataset\ndef createTokens(df):\n    return df['TEXT'].apply(lambda x: x.split(' '))\n\n#Lets create a function to drop \"Abstract_id\", \"Location\" and \"TEXT\" columns from dataset\ndef dropCols(df):\n    return df.drop(columns=['ABSTRACT_ID', 'LOCATION', 'TEXT'])\n\n# Lets create a function to remove stop words from the Text column\ndef removeStop(df):\n    stopWords = spacy.lang.en.stop_words.STOP_WORDS\n    # Remove any stopwords which appear to be an Abbreviation\n    [stopWords.remove(t) for t in df['ABV'].str.lower() if t in stopWords]\n    return df['TOKEN'].apply(lambda x: [item for item in x if not item in stopWords])\n\ndef tolower(df):\n    return [t.lower() for t in df['TEXT']]\n\ndef preProcessData(df):   \n    df['TEXT'] = tolower(df)\n    df['TEXT'] = removePunctuation(df)\n    df['TOKEN'] = createTokens(df)\n    df = dropCols(df)\n    df['TOKEN'] = removeStop(df)\n    return df\n\n# Lets load the train dataset.\ntrain = preProcessData(train)\nvalid = preProcessData(valid)\n# test = preProcessData(test)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:23:04.020680Z","iopub.execute_input":"2024-12-04T07:23:04.021029Z","iopub.status.idle":"2024-12-04T07:23:44.721704Z","shell.execute_reply.started":"2024-12-04T07:23:04.020999Z","shell.execute_reply":"2024-12-04T07:23:44.721050Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:23:53.520678Z","iopub.execute_input":"2024-12-04T07:23:53.521020Z","iopub.status.idle":"2024-12-04T07:23:53.524711Z","shell.execute_reply.started":"2024-12-04T07:23:53.520990Z","shell.execute_reply":"2024-12-04T07:23:53.523996Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# valid.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:23:54.061640Z","iopub.execute_input":"2024-12-04T07:23:54.061977Z","iopub.status.idle":"2024-12-04T07:23:54.065416Z","shell.execute_reply.started":"2024-12-04T07:23:54.061944Z","shell.execute_reply":"2024-12-04T07:23:54.064344Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:23:54.411079Z","iopub.execute_input":"2024-12-04T07:23:54.411427Z","iopub.status.idle":"2024-12-04T07:23:54.414600Z","shell.execute_reply.started":"2024-12-04T07:23:54.411398Z","shell.execute_reply":"2024-12-04T07:23:54.413936Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lets keep only relevant records in Valid and test set.","metadata":{}},{"cell_type":"code","source":"abbrev = list(train['ABV'].unique())\nvalid = valid[valid['ABV'].isin(abbrev)]\n# test = test[test['ABV'].isin(abbrev)]\nlabels = list(train['LABEL'].unique())\nvalid = valid[valid['LABEL'].isin(labels)]\n# test = test[test['LABEL'].isin(labels)]\n\n# Lets tag every Token List with its Label\n\ntrain_tagged = train.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\nvalid_tagged = valid.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\n# test_tagged = test.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\n\n# # Convert TOKEN column from string to list\n# train['TOKEN'] = train['TOKEN'].apply(lambda x: ast.literal_eval(x))\n# valid['TOKEN'] = valid['TOKEN'].apply(lambda x: ast.literal_eval(x))\n# test['TOKEN'] = test['TOKEN'].apply(lambda x: ast.literal_eval(x))","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:24:07.973366Z","iopub.execute_input":"2024-12-04T07:24:07.973707Z","iopub.status.idle":"2024-12-04T07:24:10.013090Z","shell.execute_reply.started":"2024-12-04T07:24:07.973682Z","shell.execute_reply":"2024-12-04T07:24:10.012260Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tagged.values[:2]","metadata":{"execution":{"iopub.status.busy":"2024-12-04T07:24:11.864765Z","iopub.execute_input":"2024-12-04T07:24:11.865118Z","iopub.status.idle":"2024-12-04T07:24:11.872179Z","shell.execute_reply.started":"2024-12-04T07:24:11.865090Z","shell.execute_reply":"2024-12-04T07:24:11.871355Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# Instantiate the LabelEncoder\nlabel_encoder = LabelEncoder()\n\ndef prepare_text_and_labels(dataset, include_labels=True):\n    \"\"\"\n    Prepares text variables and labels for abbreviation expansion tasks.\n    \n    Args:\n        dataset (pd.DataFrame): The dataset containing 'TOKEN' and optionally 'LABEL' columns.\n        include_labels (bool): Whether to return the labels along with the texts.\n        \n    Returns:\n        list: A list of input texts.\n        list (optional): A list of labels if include_labels is True.\n    \"\"\"\n    # Generate input texts from the 'TOKEN' column\n    texts = [\" \".join(tokens) for tokens in dataset[\"TOKEN\"]]\n    \n    # If labels are needed, extract them\n    if include_labels and \"LABEL\" in dataset.columns:\n        labels = dataset[\"LABEL\"].tolist()\n        return texts, labels\n    \n    return texts\n\n# Assuming `train`, `valid`, and `test` are pandas DataFrames from your notebook\n\n# For training set\ntrain_texts, train_labels = prepare_text_and_labels(train)\n\n# For validation set\nvalid_texts, valid_labels = prepare_text_and_labels(valid)\n\n# For test set, if you don't need labels\n# test_texts = prepare_text_and_labels(test, include_labels=False)\n\n# Display examples\nprint(train_texts[:2])  # Example texts\nprint(train_labels[:2])  # Corresponding labels\n\n# Fit the LabelEncoder on the training labels\nlabel_encoder.fit(train_labels)\n\n# Transform the training labels (if needed)\ntrain_labels_encoded = label_encoder.transform(train_labels)\n\n# Filter validation set to remove unseen labels\nvalid_filtered = valid[valid[\"LABEL\"].isin(label_encoder.classes_)]\nvalid_texts, valid_labels = prepare_text_and_labels(valid_filtered)\n\n# Transform the validation labels\nvalid_labels_encoded = label_encoder.transform(valid_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:24:23.244669Z","iopub.execute_input":"2024-12-04T07:24:23.245022Z","iopub.status.idle":"2024-12-04T07:24:23.644206Z","shell.execute_reply.started":"2024-12-04T07:24:23.244993Z","shell.execute_reply":"2024-12-04T07:24:23.643279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # from transformers import AutoTokenizer\n# # import torch\n# # from transformers import AutoModelForSequenceClassification\n# # from transformers import TrainingArguments\n# # from transformers import Trainer\n\n\n# # # Load tokenizer\n# # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# # # Tokenize training and validation datasets\n# # train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n# # valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n\n\n# # class AbbreviationDataset(torch.utils.data.Dataset):\n# #     def __init__(self, encodings, labels=None):\n# #         self.encodings = encodings\n# #         self.labels = labels\n\n# #     def __len__(self):\n# #         return len(self.encodings[\"input_ids\"])\n\n# #     def __getitem__(self, idx):\n# #         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n# #         if self.labels is not None:\n# #             item[\"labels\"] = torch.tensor(self.labels[idx])\n# #         return item\n\n# # # Create datasets\n# # train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n# # valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n\n\n\n# # # Load pretrained model\n# # num_labels = len(label_encoder.classes_)  # Number of unique labels\n# # model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n\n\n\n# # training_args = TrainingArguments(\n# #     output_dir=\"./results\",           # Directory to save model checkpoints\n# #     evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n# #     learning_rate=2e-5,              # Learning rate\n# #     per_device_train_batch_size=16,  # Batch size for training\n# #     per_device_eval_batch_size=16,   # Batch size for evaluation\n# #     num_train_epochs=3,              # Number of epochs\n# #     weight_decay=0.01,               # Weight decay for regularization\n# #     save_total_limit=1,              # Save only the last checkpoint\n# #     logging_dir=\"./logs\",            # Directory for logs\n# #     logging_steps=10,                # Log every 10 steps\n# # )\n\n\n\n# # trainer = Trainer(\n# #     model=model,\n# #     args=training_args,\n# #     train_dataset=train_dataset,\n# #     eval_dataset=valid_dataset,\n# #     tokenizer=tokenizer,\n# # )\n\n# # # Train the model\n# # trainer.train()\n\n# # results = trainer.evaluate()\n# # print(\"Validation Results:\", results)\n\n# # model.save_pretrained(\"./trained_model\")\n# # tokenizer.save_pretrained(\"./trained_model\")\n\n\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n# import torch\n# from sklearn.preprocessing import LabelEncoder\n\n# def train_abbreviation_model(train_texts, train_labels, valid_texts, valid_labels, model_name=\"bert-base-uncased\", output_dir=\"./trained_model\", num_epochs=3, batch_size=16, learning_rate=2e-5):\n#     \"\"\"\n#     Trains a BERT-based model for abbreviation expansion.\n    \n#     Args:\n#         train_texts (list): List of training texts.\n#         train_labels (list): List of training labels.\n#         valid_texts (list): List of validation texts.\n#         valid_labels (list): List of validation labels.\n#         model_name (str): Pretrained model name from Hugging Face.\n#         output_dir (str): Directory to save the trained model.\n#         num_epochs (int): Number of epochs for training.\n#         batch_size (int): Batch size for training and evaluation.\n#         learning_rate (float): Learning rate for the optimizer.\n    \n#     Returns:\n#         dict: Evaluation results from the validation dataset.\n#     \"\"\"\n#     # Tokenizer\n#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n#     # Tokenize datasets\n#     train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n#     valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n    \n#     # Encode labels\n#     label_encoder = LabelEncoder()\n#     train_labels_encoded = label_encoder.fit_transform(train_labels)\n#     valid_labels_encoded = label_encoder.transform(valid_labels)\n    \n#     # Create PyTorch datasets\n#     class AbbreviationDataset(torch.utils.data.Dataset):\n#         def __init__(self, encodings, labels=None):\n#             self.encodings = encodings\n#             self.labels = labels\n\n#         def __len__(self):\n#             return len(self.encodings[\"input_ids\"])\n\n#         def __getitem__(self, idx):\n#             item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n#             if self.labels is not None:\n#                 item[\"labels\"] = torch.tensor(self.labels[idx])\n#             return item\n\n#     train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n#     valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n    \n#     Load pretrained model\n#     num_labels = len(label_encoder.classes_)\n#     model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n    \n#     # Training arguments\n#     training_args = TrainingArguments(\n#         output_dir=output_dir,\n#         evaluation_strategy=\"epoch\",\n#         learning_rate=learning_rate,\n#         per_device_train_batch_size=batch_size,\n#         per_device_eval_batch_size=batch_size,\n#         num_train_epochs=num_epochs,\n#         weight_decay=0.01,\n#         save_total_limit=1,\n#         logging_dir=f\"{output_dir}/logs\",\n#         logging_steps=10,\n#     )\n    \n#     # Trainer\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=train_dataset,\n#         eval_dataset=valid_dataset,\n#         tokenizer=tokenizer,\n#     )\n    \n#     # Train the model\n#     trainer.train()\n    \n#     # Evaluate the model\n#     results = trainer.evaluate()\n#     # print(\"Validation Results:\", results)\n    \n#     # Save the model and tokenizer\n#     model.save_pretrained(output_dir)\n#     tokenizer.save_pretrained(output_dir)\n#     print(f\"Model and tokenizer saved to {output_dir}\")\n    \n#     return results\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:18:56.600557Z","iopub.execute_input":"2024-12-04T08:18:56.600919Z","iopub.status.idle":"2024-12-04T08:18:56.613618Z","shell.execute_reply.started":"2024-12-04T08:18:56.600889Z","shell.execute_reply":"2024-12-04T08:18:56.612904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_results, trainer, model, training_args, learning_rate = train_abbreviation_model(\n#     train_texts=train_texts,\n#     train_labels=train_labels,\n#     valid_texts=valid_texts,\n#     valid_labels=valid_labels,\n#     model_name=\"bert-base-uncased\",\n#     output_dir=\"./trained_model\",\n#     num_epochs=3,\n#     batch_size=16,\n#     learning_rate=2e-5,\n# )\n\n# print(\"Training completed. Validation results:\")\n# print(train_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:18:57.236462Z","iopub.execute_input":"2024-12-04T08:18:57.236752Z","iopub.status.idle":"2024-12-04T08:23:01.830214Z","shell.execute_reply.started":"2024-12-04T08:18:57.236726Z","shell.execute_reply":"2024-12-04T08:23:01.828836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\nimport numpy as np\n\ndef train_and_evaluate_abbreviation_model(train_texts, train_labels, valid_filtered, model_name=\"bert-base-uncased\", output_dir=\"./trained_model\", num_epochs=3, batch_size=16, learning_rate=2e-5):\n    \"\"\"\n    Trains a BERT-based model for abbreviation expansion and evaluates on a filtered validation dataset.\n    \n    Args:\n        train_texts (list): List of training texts.\n        train_labels (list): List of training labels.\n        valid_filtered (pd.DataFrame): Filtered validation dataset with 'TOKEN' and 'LABEL' columns.\n        model_name (str): Pretrained model name from Hugging Face.\n        output_dir (str): Directory to save the trained model.\n        num_epochs (int): Number of epochs for training.\n        batch_size (int): Batch size for training and evaluation.\n        learning_rate (float): Learning rate for the optimizer.\n    \n    Returns:\n        dict: Evaluation metrics including precision, recall, F1-score, and accuracy.\n    \"\"\"\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Tokenize training data\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n    \n    # Encode labels for training\n    label_encoder = LabelEncoder()\n    train_labels_encoded = label_encoder.fit_transform(train_labels)\n    \n    # Prepare validation texts and labels\n    valid_texts, valid_labels = prepare_text_and_labels(valid_filtered)\n    \n    # Tokenize validation data\n    valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n    valid_labels_encoded = label_encoder.transform(valid_labels)\n    \n    # Create PyTorch datasets\n    class AbbreviationDataset(torch.utils.data.Dataset):\n        def __init__(self, encodings, labels=None):\n            self.encodings = encodings\n            self.labels = labels\n\n        def __len__(self):\n            return len(self.encodings[\"input_ids\"])\n\n        def __getitem__(self, idx):\n            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n            if self.labels is not None:\n                item[\"labels\"] = torch.tensor(self.labels[idx])\n            return item\n\n    train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n    valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n    \n    # Load pretrained model\n    num_labels = len(label_encoder.classes_)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        evaluation_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        save_total_limit=1,\n        logging_dir=f\"{output_dir}/logs\",\n        logging_steps=10,\n    )\n    \n    # Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        tokenizer=tokenizer,\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    # Evaluate the model on the validation set\n    valid_preds = trainer.predict(valid_dataset)\n    \n    # Extract true and predicted labels\n    predicted_labels = torch.argmax(torch.tensor(valid_preds.predictions), axis=1).numpy()\n    true_labels = valid_labels_encoded\n    \n    # Compute evaluation metrics\n    precision = precision_score(true_labels, predicted_labels, average='weighted')\n    recall = recall_score(true_labels, predicted_labels, average='weighted')\n    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    \n    # Confusion matrix and classification report\n    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n    class_report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)\n    \n    # Save the model and tokenizer\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Model and tokenizer saved to {output_dir}\")\n    \n    # Return metrics and other results\n    metrics = {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1,\n        \"confusion_matrix\": conf_matrix,\n        \"classification_report\": class_report\n    }\n    \n    return metrics\n\n\n\nmetrics = train_and_evaluate_abbreviation_model(\n    train_texts=train_texts,\n    train_labels=train_labels,\n    valid_filtered=valid_filtered,  # Validation data filtered for unseen labels\n    model_name=\"bert-base-uncased\",\n    output_dir=\"./trained_model\",\n    num_epochs=3,\n    batch_size=16,\n    learning_rate=2e-5,\n)\n\n# Display metrics\nprint(\"Evaluation Metrics:\")\nfor key, value in metrics.items():\n    if key == \"confusion_matrix\":\n        print(f\"{key}:\\n{value}\")\n    else:\n        print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:23:16.222126Z","iopub.execute_input":"2024-12-04T08:23:16.222414Z","iopub.status.idle":"2024-12-04T09:06:13.313720Z","shell.execute_reply.started":"2024-12-04T08:23:16.222391Z","shell.execute_reply":"2024-12-04T09:06:13.313006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test with valid dataset","metadata":{}},{"cell_type":"code","source":"# valid_texts, valid_labels = prepare_text_and_labels(valid_filtered)\n\n# # Tokenize the validation data\n# valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n# valid_labels_encoded = label_encoder.transform(valid_labels)\n\n# # Convert to PyTorch Dataseta\n# valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n\n# # Get predictions on the tokenized validation dataset\n# valid_preds = trainer.predict(valid_dataset)\n\n# # Extract predicted labels\n# predicted_labels = torch.argmax(torch.tensor(valid_preds.predictions), axis=1).numpy()\n# true_labels = valid_labels_encoded\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:16:51.030334Z","iopub.execute_input":"2024-12-04T08:16:51.030645Z","iopub.status.idle":"2024-12-04T08:16:51.133930Z","shell.execute_reply.started":"2024-12-04T08:16:51.030619Z","shell.execute_reply":"2024-12-04T08:16:51.132766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Unique True Labels:\", len(set(true_labels)), sorted(set(true_labels)))\nprint(\"Unique Predicted Labels:\", len(set(predicted_labels)), sorted(set(predicted_labels)))\nprint(\"Classes in Label Encoder:\", len(label_encoder.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T09:06:28.447756Z","iopub.execute_input":"2024-12-04T09:06:28.448113Z","iopub.status.idle":"2024-12-04T09:06:28.473863Z","shell.execute_reply.started":"2024-12-04T09:06:28.448082Z","shell.execute_reply":"2024-12-04T09:06:28.472248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine unique classes from true and predicted labels\nrelevant_classes = sorted(set(true_labels).union(set(predicted_labels)))\n\n# Subset the target names to match relevant classes\nrelevant_target_names = [label_encoder.classes_[i] for i in relevant_classes]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:07:49.976156Z","iopub.status.idle":"2024-12-04T08:07:49.976520Z","shell.execute_reply":"2024-12-04T08:07:49.976327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import confusion_matrix, classification_report\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# # Compute the confusion matrix\n# conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=relevant_classes)\n\n# # Plot the confusion matrix\n# plt.figure(figsize=(12, 10))\n# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=relevant_target_names, yticklabels=relevant_target_names)\n# plt.xlabel(\"Predicted Labels\")\n# plt.ylabel(\"True Labels\")\n# plt.title(\"Confusion Matrix\")\n# plt.show()\n\n# # Generate classification report\n# print(classification_report(true_labels, predicted_labels, target_names=relevant_target_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:07:49.977423Z","iopub.status.idle":"2024-12-04T08:07:49.977797Z","shell.execute_reply":"2024-12-04T08:07:49.977611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Compute metrics\nprecision = precision_score(true_labels, predicted_labels, average='weighted')  # or 'macro', 'micro'\nrecall = recall_score(true_labels, predicted_labels, average='weighted')        # or 'macro', 'micro'\nf1 = f1_score(true_labels, predicted_labels, average='weighted')                # or 'macro', 'micro'\naccuracy = accuracy_score(true_labels, predicted_labels)\n\n# Display results\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision (Weighted): {precision:.4f}\")\nprint(f\"Recall (Weighted): {recall:.4f}\")\nprint(f\"F1 Score (Weighted): {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T08:07:49.978790Z","iopub.status.idle":"2024-12-04T08:07:49.979241Z","shell.execute_reply":"2024-12-04T08:07:49.979021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}