{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbreviation Disambiguation in Medical Texts - Data Modeling\n",
    "\n",
    "This Notebook is in continuation of the notebook- 'Step 2- Data Preprocessing' and lists down:\n",
    "\n",
    "1. Modeling Preprocessed data using: GridSearchCV on Logistic Regression, SVM and XG Boost.\n",
    "2. Testing the models using Test set.\n",
    "3. Comparing the models and identifying the Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step# 1: Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:20:34.817431Z",
     "iopub.status.busy": "2024-12-04T07:20:34.817100Z",
     "iopub.status.idle": "2024-12-04T07:20:37.775566Z",
     "shell.execute_reply": "2024-12-04T07:20:37.774905Z",
     "shell.execute_reply.started": "2024-12-04T07:20:34.817358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Importing the Required Python Packages\n",
    "import string\n",
    "import pandas as pd\n",
    "import ast\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import spacy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:20:37.777634Z",
     "iopub.status.busy": "2024-12-04T07:20:37.777381Z",
     "iopub.status.idle": "2024-12-04T07:22:09.092125Z",
     "shell.execute_reply": "2024-12-04T07:22:09.091388Z",
     "shell.execute_reply.started": "2024-12-04T07:20:37.777609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lets load the default english model of spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "datapath = \"../input/medal-emnlp/pretrain_subset\"\n",
    "# datapath = \"archive/pretrain_subset\"\n",
    "\n",
    "# Lets load the train dataset.\n",
    "\n",
    "train = pd.read_csv(datapath + '/train.csv')\n",
    "# train = train[:100000]\n",
    "\n",
    "# Lets load validation and test datasets as well\n",
    "valid = pd.read_csv(f'{datapath}/valid.csv')\n",
    "# test = pd.read_csv(f'{datapath}/test.csv')\n",
    "# valid = valid[:10000]\n",
    "# test = test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:23:04.021029Z",
     "iopub.status.busy": "2024-12-04T07:23:04.020680Z",
     "iopub.status.idle": "2024-12-04T07:23:44.721704Z",
     "shell.execute_reply": "2024-12-04T07:23:44.721050Z",
     "shell.execute_reply.started": "2024-12-04T07:23:04.020999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lets create a function to create a new feature 'ABV' from dataset\n",
    "def createFeature(df):    \n",
    "    return [x.split(' ')[y] for x,y in zip(df['TEXT'], df['LOCATION'])]\n",
    "\n",
    "train['ABV'] = createFeature(train)\n",
    "valid['ABV'] = createFeature(valid)\n",
    "# test['ABV'] = createFeature(test)\n",
    "\n",
    "grouped = train.groupby(by=['ABV', 'LABEL'], as_index = False, sort = False).count()\n",
    "grouped = grouped.sort_values(by='TEXT', ascending = False)\n",
    "\n",
    "# topAbv = grouped['ABV'][:20]\n",
    "\n",
    "train = train[train['ABV'].isin(topAbv)]\n",
    "valid = valid[valid['ABV'].isin(topAbv)]\n",
    "# test = test[test['ABV'].isin(topAbv)]\n",
    "\n",
    "# Lets create a function to remove all the Punctuations from Text\n",
    "def removePunctuation(df):\n",
    "    return [t.translate(str.maketrans('','',string.punctuation)) for t in df['TEXT']]\n",
    "\n",
    "# Lets create a function to Tokenize the Text column of dataset\n",
    "def createTokens(df):\n",
    "    return df['TEXT'].apply(lambda x: x.split(' '))\n",
    "\n",
    "#Lets create a function to drop \"Abstract_id\", \"Location\" and \"TEXT\" columns from dataset\n",
    "def dropCols(df):\n",
    "    return df.drop(columns=['ABSTRACT_ID', 'LOCATION', 'TEXT'])\n",
    "\n",
    "# Lets create a function to remove stop words from the Text column\n",
    "def removeStop(df):\n",
    "    stopWords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    # Remove any stopwords which appear to be an Abbreviation\n",
    "    [stopWords.remove(t) for t in df['ABV'].str.lower() if t in stopWords]\n",
    "    return df['TOKEN'].apply(lambda x: [item for item in x if not item in stopWords])\n",
    "\n",
    "def tolower(df):\n",
    "    return [t.lower() for t in df['TEXT']]\n",
    "\n",
    "def preProcessData(df):   \n",
    "    df['TEXT'] = tolower(df)\n",
    "    df['TEXT'] = removePunctuation(df)\n",
    "    df['TOKEN'] = createTokens(df)\n",
    "    df = dropCols(df)\n",
    "    df['TOKEN'] = removeStop(df)\n",
    "    return df\n",
    "\n",
    "# Lets load the train dataset.\n",
    "train = preProcessData(train)\n",
    "valid = preProcessData(valid)\n",
    "# test = preProcessData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:23:53.521020Z",
     "iopub.status.busy": "2024-12-04T07:23:53.520678Z",
     "iopub.status.idle": "2024-12-04T07:23:53.524711Z",
     "shell.execute_reply": "2024-12-04T07:23:53.523996Z",
     "shell.execute_reply.started": "2024-12-04T07:23:53.520990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:23:54.061977Z",
     "iopub.status.busy": "2024-12-04T07:23:54.061640Z",
     "iopub.status.idle": "2024-12-04T07:23:54.065416Z",
     "shell.execute_reply": "2024-12-04T07:23:54.064344Z",
     "shell.execute_reply.started": "2024-12-04T07:23:54.061944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# valid.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:23:54.411427Z",
     "iopub.status.busy": "2024-12-04T07:23:54.411079Z",
     "iopub.status.idle": "2024-12-04T07:23:54.414600Z",
     "shell.execute_reply": "2024-12-04T07:23:54.413936Z",
     "shell.execute_reply.started": "2024-12-04T07:23:54.411398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets keep only relevant records in Valid and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:24:07.973707Z",
     "iopub.status.busy": "2024-12-04T07:24:07.973366Z",
     "iopub.status.idle": "2024-12-04T07:24:10.013090Z",
     "shell.execute_reply": "2024-12-04T07:24:10.012260Z",
     "shell.execute_reply.started": "2024-12-04T07:24:07.973682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "abbrev = list(train['ABV'].unique())\n",
    "valid = valid[valid['ABV'].isin(abbrev)]\n",
    "# test = test[test['ABV'].isin(abbrev)]\n",
    "labels = list(train['LABEL'].unique())\n",
    "valid = valid[valid['LABEL'].isin(labels)]\n",
    "# test = test[test['LABEL'].isin(labels)]\n",
    "\n",
    "# Lets tag every Token List with its Label\n",
    "\n",
    "train_tagged = train.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\n",
    "valid_tagged = valid.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\n",
    "# test_tagged = test.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\n",
    "\n",
    "# # Convert TOKEN column from string to list\n",
    "# train['TOKEN'] = train['TOKEN'].apply(lambda x: ast.literal_eval(x))\n",
    "# valid['TOKEN'] = valid['TOKEN'].apply(lambda x: ast.literal_eval(x))\n",
    "# test['TOKEN'] = test['TOKEN'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:24:11.865118Z",
     "iopub.status.busy": "2024-12-04T07:24:11.864765Z",
     "iopub.status.idle": "2024-12-04T07:24:11.872179Z",
     "shell.execute_reply": "2024-12-04T07:24:11.871355Z",
     "shell.execute_reply.started": "2024-12-04T07:24:11.865090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([TaggedDocument(words=['yearold', 'girl', 'presented', 'unilateral', 'c6', 'nerve', 'palsy', 'papilledema', 't3', 'severe', 'ha', 'magnetic', 'resonance', 'imaging', 'showed', 'thrombosis', 'superior', 'sagittal', 'sinus', 'pt', 'right', 'transverse', 'sinus', 'attributed', 'oral', 'contraceptive', 'use', 't3', 'coagulation', 'workup', 'negative', 'svr', 'anticoagulation', 'caused', 'hemorrhagic', 'papillopathy', 'eyes', 'raising', 'question', 'ac', 'discontinued', 'diagnostic', 'management', 'issues', 'cbf', 'venous', 'sinus', 'thrombosis', 'secondary', 'intracranial', 'hypertension', 'discussed'], tags=['anticoagulation']),\n",
       "       TaggedDocument(words=['mvr', 'procedure', 'choice', 'treat', 'mitral', 'valve', 'dysfunction', 'advantages', 'mvr', 'mitral', 'vr', 'include', 'improved', 'lts', 'better', 'preservation', 'left', 'vvi', 'cf', 'greater', 'freedom', 'endocarditis', 'thromboembolism', 'anticoagulantrelated', 'hemorrhage', 'feasibility', 'durability', 'mvr', 'depend', 'etiology', 'mitral', 'valve', 'dysfunction', 'degenerative', 'icm', 'mitral', 'valve', 'diseases', 'valve', 'repair', 'possible', 'cases', 'freedom', 'reoperation', 'high', 'contrast', 'rheumatic', 'valves', 'amenable', 'repair', 'durability', 'limited', 'isolated', 'valve', 'operations', 'performed', 'minimally', 'invasive', 'fashion', 'partial', 'upper', 'sternotomy', 'patients', 'atrial', 'fibrillation', 'mvd', 'maze', 'procedure', 'pulmonary', 'vein', 'isolation', 'performed', 'eliminating', 'atrial', 'fibrillation', 'need', 'lt', 'ac'], tags=['anticoagulation'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:24:23.245022Z",
     "iopub.status.busy": "2024-12-04T07:24:23.244669Z",
     "iopub.status.idle": "2024-12-04T07:24:23.644206Z",
     "shell.execute_reply": "2024-12-04T07:24:23.643279Z",
     "shell.execute_reply.started": "2024-12-04T07:24:23.244993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yearold girl presented unilateral c6 nerve palsy papilledema t3 severe ha magnetic resonance imaging showed thrombosis superior sagittal sinus pt right transverse sinus attributed oral contraceptive use t3 coagulation workup negative svr anticoagulation caused hemorrhagic papillopathy eyes raising question ac discontinued diagnostic management issues cbf venous sinus thrombosis secondary intracranial hypertension discussed', 'mvr procedure choice treat mitral valve dysfunction advantages mvr mitral vr include improved lts better preservation left vvi cf greater freedom endocarditis thromboembolism anticoagulantrelated hemorrhage feasibility durability mvr depend etiology mitral valve dysfunction degenerative icm mitral valve diseases valve repair possible cases freedom reoperation high contrast rheumatic valves amenable repair durability limited isolated valve operations performed minimally invasive fashion partial upper sternotomy patients atrial fibrillation mvd maze procedure pulmonary vein isolation performed eliminating atrial fibrillation need lt ac']\n",
      "['anticoagulation', 'anticoagulation']\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# Instantiate the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "def prepare_text_and_labels(dataset, include_labels=True):\n",
    "    \"\"\"\n",
    "    Prepares text variables and labels for abbreviation expansion tasks.\n",
    "    \n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing 'TOKEN' and optionally 'LABEL' columns.\n",
    "        include_labels (bool): Whether to return the labels along with the texts.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of input texts.\n",
    "        list (optional): A list of labels if include_labels is True.\n",
    "    \"\"\"\n",
    "    # Generate input texts from the 'TOKEN' column\n",
    "    texts = [\" \".join(tokens) for tokens in dataset[\"TOKEN\"]]\n",
    "    \n",
    "    # If labels are needed, extract them\n",
    "    if include_labels and \"LABEL\" in dataset.columns:\n",
    "        labels = dataset[\"LABEL\"].tolist()\n",
    "        return texts, labels\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Assuming `train`, `valid`, and `test` are pandas DataFrames from your notebook\n",
    "\n",
    "# For training set\n",
    "train_texts, train_labels = prepare_text_and_labels(train)\n",
    "\n",
    "# For validation set\n",
    "valid_texts, valid_labels = prepare_text_and_labels(valid)\n",
    "\n",
    "# For test set, if you don't need labels\n",
    "# test_texts = prepare_text_and_labels(test, include_labels=False)\n",
    "\n",
    "# Display examples\n",
    "print(train_texts[:2])  # Example texts\n",
    "print(train_labels[:2])  # Corresponding labels\n",
    "\n",
    "# Fit the LabelEncoder on the training labels\n",
    "label_encoder.fit(train_labels)\n",
    "\n",
    "# Transform the training labels (if needed)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "\n",
    "# Filter validation set to remove unseen labels\n",
    "valid_filtered = valid[valid[\"LABEL\"].isin(label_encoder.classes_)]\n",
    "valid_texts, valid_labels = prepare_text_and_labels(valid_filtered)\n",
    "\n",
    "# Transform the validation labels\n",
    "valid_labels_encoded = label_encoder.transform(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T08:18:56.600919Z",
     "iopub.status.busy": "2024-12-04T08:18:56.600557Z",
     "iopub.status.idle": "2024-12-04T08:18:56.613618Z",
     "shell.execute_reply": "2024-12-04T08:18:56.612904Z",
     "shell.execute_reply.started": "2024-12-04T08:18:56.600889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # from transformers import AutoTokenizer\n",
    "# # import torch\n",
    "# # from transformers import AutoModelForSequenceClassification\n",
    "# # from transformers import TrainingArguments\n",
    "# # from transformers import Trainer\n",
    "\n",
    "\n",
    "# # # Load tokenizer\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # # Tokenize training and validation datasets\n",
    "# # train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "# # valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "\n",
    "# # class AbbreviationDataset(torch.utils.data.Dataset):\n",
    "# #     def __init__(self, encodings, labels=None):\n",
    "# #         self.encodings = encodings\n",
    "# #         self.labels = labels\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "# #         if self.labels is not None:\n",
    "# #             item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "# #         return item\n",
    "\n",
    "# # # Create datasets\n",
    "# # train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n",
    "# # valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n",
    "\n",
    "\n",
    "\n",
    "# # # Load pretrained model\n",
    "# # num_labels = len(label_encoder.classes_)  # Number of unique labels\n",
    "# # model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "\n",
    "\n",
    "# # training_args = TrainingArguments(\n",
    "# #     output_dir=\"./results\",           # Directory to save model checkpoints\n",
    "# #     evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "# #     learning_rate=2e-5,              # Learning rate\n",
    "# #     per_device_train_batch_size=16,  # Batch size for training\n",
    "# #     per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "# #     num_train_epochs=3,              # Number of epochs\n",
    "# #     weight_decay=0.01,               # Weight decay for regularization\n",
    "# #     save_total_limit=1,              # Save only the last checkpoint\n",
    "# #     logging_dir=\"./logs\",            # Directory for logs\n",
    "# #     logging_steps=10,                # Log every 10 steps\n",
    "# # )\n",
    "\n",
    "\n",
    "\n",
    "# # trainer = Trainer(\n",
    "# #     model=model,\n",
    "# #     args=training_args,\n",
    "# #     train_dataset=train_dataset,\n",
    "# #     eval_dataset=valid_dataset,\n",
    "# #     tokenizer=tokenizer,\n",
    "# # )\n",
    "\n",
    "# # # Train the model\n",
    "# # trainer.train()\n",
    "\n",
    "# # results = trainer.evaluate()\n",
    "# # print(\"Validation Results:\", results)\n",
    "\n",
    "# # model.save_pretrained(\"./trained_model\")\n",
    "# # tokenizer.save_pretrained(\"./trained_model\")\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# import torch\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def train_abbreviation_model(train_texts, train_labels, valid_texts, valid_labels, model_name=\"bert-base-uncased\", output_dir=\"./trained_model\", num_epochs=3, batch_size=16, learning_rate=2e-5):\n",
    "#     \"\"\"\n",
    "#     Trains a BERT-based model for abbreviation expansion.\n",
    "    \n",
    "#     Args:\n",
    "#         train_texts (list): List of training texts.\n",
    "#         train_labels (list): List of training labels.\n",
    "#         valid_texts (list): List of validation texts.\n",
    "#         valid_labels (list): List of validation labels.\n",
    "#         model_name (str): Pretrained model name from Hugging Face.\n",
    "#         output_dir (str): Directory to save the trained model.\n",
    "#         num_epochs (int): Number of epochs for training.\n",
    "#         batch_size (int): Batch size for training and evaluation.\n",
    "#         learning_rate (float): Learning rate for the optimizer.\n",
    "    \n",
    "#     Returns:\n",
    "#         dict: Evaluation results from the validation dataset.\n",
    "#     \"\"\"\n",
    "#     # Tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "#     # Tokenize datasets\n",
    "#     train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "#     valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "#     # Encode labels\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "#     valid_labels_encoded = label_encoder.transform(valid_labels)\n",
    "    \n",
    "#     # Create PyTorch datasets\n",
    "#     class AbbreviationDataset(torch.utils.data.Dataset):\n",
    "#         def __init__(self, encodings, labels=None):\n",
    "#             self.encodings = encodings\n",
    "#             self.labels = labels\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "#         def __getitem__(self, idx):\n",
    "#             item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#             if self.labels is not None:\n",
    "#                 item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "#             return item\n",
    "\n",
    "#     train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n",
    "#     valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n",
    "    \n",
    "#     Load pretrained model\n",
    "#     num_labels = len(label_encoder.classes_)\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "#     # Training arguments\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         learning_rate=learning_rate,\n",
    "#         per_device_train_batch_size=batch_size,\n",
    "#         per_device_eval_batch_size=batch_size,\n",
    "#         num_train_epochs=num_epochs,\n",
    "#         weight_decay=0.01,\n",
    "#         save_total_limit=1,\n",
    "#         logging_dir=f\"{output_dir}/logs\",\n",
    "#         logging_steps=10,\n",
    "#     )\n",
    "    \n",
    "#     # Trainer\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=valid_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#     )\n",
    "    \n",
    "#     # Train the model\n",
    "#     trainer.train()\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     results = trainer.evaluate()\n",
    "#     # print(\"Validation Results:\", results)\n",
    "    \n",
    "#     # Save the model and tokenizer\n",
    "#     model.save_pretrained(output_dir)\n",
    "#     tokenizer.save_pretrained(output_dir)\n",
    "#     print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T08:18:57.236752Z",
     "iopub.status.busy": "2024-12-04T08:18:57.236462Z",
     "iopub.status.idle": "2024-12-04T08:23:01.830214Z",
     "shell.execute_reply": "2024-12-04T08:23:01.828836Z",
     "shell.execute_reply.started": "2024-12-04T08:18:57.236726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='971' max='9861' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 971/9861 03:41 < 33:51, 4.38 it/s, Epoch 0.30/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c5a515e077a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-5796a3d1da3b>\u001b[0m in \u001b[0;36mtrain_abbreviation_model\u001b[0;34m(train_texts, train_labels, valid_texts, valid_labels, model_name, output_dir, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    919\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_results, trainer, model, training_args, learning_rate = train_abbreviation_model(\n",
    "#     train_texts=train_texts,\n",
    "#     train_labels=train_labels,\n",
    "#     valid_texts=valid_texts,\n",
    "#     valid_labels=valid_labels,\n",
    "#     model_name=\"bert-base-uncased\",\n",
    "#     output_dir=\"./trained_model\",\n",
    "#     num_epochs=3,\n",
    "#     batch_size=16,\n",
    "#     learning_rate=2e-5,\n",
    "# )\n",
    "\n",
    "# print(\"Training completed. Validation results:\")\n",
    "# print(train_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T08:23:16.222414Z",
     "iopub.status.busy": "2024-12-04T08:23:16.222126Z",
     "iopub.status.idle": "2024-12-04T09:06:13.313720Z",
     "shell.execute_reply": "2024-12-04T09:06:13.313006Z",
     "shell.execute_reply.started": "2024-12-04T08:23:16.222391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='9861' max='9861' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9861/9861 41:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.785100</td>\n",
       "      <td>2.471378</td>\n",
       "      <td>68.589400</td>\n",
       "      <td>255.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.664100</td>\n",
       "      <td>1.628383</td>\n",
       "      <td>68.625500</td>\n",
       "      <td>255.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.445900</td>\n",
       "      <td>1.454005</td>\n",
       "      <td>68.616400</td>\n",
       "      <td>255.420000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1096' max='1096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1096/1096 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./trained_model\n",
      "Evaluation Metrics:\n",
      "accuracy: 0.697763323062878\n",
      "precision: 0.673149790509698\n",
      "recall: 0.697763323062878\n",
      "f1_score: 0.6734324139880631\n",
      "confusion_matrix:\n",
      "[[ 5  0  0 ...  0  3  0]\n",
      " [ 0 80  0 ...  0  0  0]\n",
      " [ 0  0 64 ...  1  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 69  2  0]\n",
      " [ 0  0  0 ...  0 57  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "classification_report:                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                  aberrant crypts       1.00      0.17      0.29        29\n",
      "                                 absence epilepsy       0.95      0.90      0.92        89\n",
      "                                   absolute error       0.67      0.72      0.69        89\n",
      "                                       acebutolol       0.91      0.92      0.92        89\n",
      "                                          acetone       0.57      0.75      0.65        89\n",
      "                                  acid ceramidase       0.83      0.85      0.84        41\n",
      "                                acoustic emission       0.92      0.95      0.94        77\n",
      "                                 acridinium ester       0.00      0.00      0.00        16\n",
      "                     acrodermatitis enteropathica       0.81      0.73      0.77        48\n",
      "                                acromioclavicular       0.86      0.89      0.87        89\n",
      "                                 activated carbon       0.65      0.84      0.74        89\n",
      "                               activated charcoal       0.76      0.70      0.73        90\n",
      "                           activation coefficient       0.00      0.00      0.00        12\n",
      "                             activity coefficient       0.71      0.38      0.49        58\n",
      "                              acute cholecystitis       0.84      0.88      0.86        89\n",
      "                                   acute diarrhea       0.79      0.86      0.82        90\n",
      "                               acute exacerbation       0.74      0.82      0.78        89\n",
      "                                   adherent cells       0.69      0.66      0.68        89\n",
      "                                            adult       0.53      0.18      0.27        90\n",
      "                                    adultdirected       0.93      0.76      0.84        17\n",
      "                                 aerobic exercise       0.83      0.82      0.82        89\n",
      "                              aesthetic component       1.00      0.82      0.90        11\n",
      "                              affective disorders       0.56      0.64      0.60        89\n",
      "                          affinity chromatography       0.55      0.60      0.57        89\n",
      "                                   afterdischarge       0.64      0.78      0.70        89\n",
      "                   against platelet glycoproteins       0.00      0.00      0.00         4\n",
      "                                   air conduction       0.87      0.90      0.88        58\n",
      "                               alcohol dependence       0.87      0.80      0.83        89\n",
      "                                 aldrin epoxidase       0.00      0.00      0.00        15\n",
      "                          allergic conjunctivitis       0.79      0.90      0.84        89\n",
      "                                   allyl chloride       0.00      0.00      0.00        14\n",
      "                                    allylestrenol       0.70      0.44      0.54        16\n",
      "                                      alternating       0.67      0.35      0.46        89\n",
      "                          alveolar echinococcosis       0.81      0.88      0.84        90\n",
      "                      alzheimers disease patients       0.52      0.63      0.57        90\n",
      "                           alzheimertype dementia       0.00      0.00      0.00        42\n",
      "                                    amniocentesis       0.90      0.93      0.92        89\n",
      "                               amygdaloid complex       0.64      0.80      0.71        89\n",
      "                                      anchor cell       0.94      0.89      0.92        19\n",
      "                             androgen deprivation       0.92      0.85      0.88        89\n",
      "                                   anion exchange       0.62      0.74      0.67        89\n",
      "                            anoxic depolarization       0.00      0.00      0.00        23\n",
      "                                 anterior chamber       0.75      0.87      0.81        89\n",
      "                              anterior commissure       0.70      0.59      0.64        88\n",
      "                                     anterodorsal       0.69      0.65      0.67        71\n",
      "                    anterodorsal thalamic nucleus       0.00      0.00      0.00         5\n",
      "                                    anticoagulant       0.54      0.38      0.45        89\n",
      "                                  anticoagulation       0.53      0.72      0.61        89\n",
      "                                 antiglycoprotein       0.00      0.00      0.00        26\n",
      "                              aortic constriction       0.76      0.83      0.79        88\n",
      "                                  aortic diameter       0.81      0.63      0.71        90\n",
      "                                aortic dissection       0.74      0.88      0.80        89\n",
      "                                   apatite cement       0.00      0.00      0.00        11\n",
      "                           apparent digestibility       0.69      0.87      0.77        89\n",
      "                                  aqueous extract       0.79      0.87      0.83        89\n",
      "                                  arachnoid cysts       0.89      0.90      0.89        89\n",
      "                                arm circumference       0.93      0.84      0.88        90\n",
      "                              arrhythmogenic dose       0.00      0.00      0.00         8\n",
      "                              arterial compliance       0.75      0.72      0.74        89\n",
      "                              articular cartilage       0.76      0.84      0.80        89\n",
      "                           artificial circulation       0.67      0.07      0.12        30\n",
      "                                  asbestos cement       1.00      0.90      0.95        20\n",
      "                          assimilation efficiency       0.78      0.58      0.67        31\n",
      "                                 atherogenic diet       0.79      0.80      0.80        90\n",
      "                                atopic dermatitis       0.69      0.86      0.77        90\n",
      "                                    atopic eczema       0.91      0.82      0.87        90\n",
      "                           attenuation correction       0.68      0.93      0.79        90\n",
      "                             attenuationcorrected       0.00      0.00      0.00        18\n",
      "                               atypical carcinoid       0.85      0.90      0.88        59\n",
      "                              atypical depression       0.69      0.62      0.65        60\n",
      "                              autoimmune diseases       0.62      0.69      0.65        89\n",
      "                            autonomic dysfunction       0.50      0.56      0.53        90\n",
      "                            autonomic dysreflexia       0.86      0.92      0.89        83\n",
      "                               autosomal dominant       0.64      0.78      0.70        89\n",
      "                          avian encephalomyelitis       0.00      0.00      0.00        10\n",
      "                               axillary clearance       0.87      0.87      0.87        39\n",
      "                              axillary dissection       0.80      0.91      0.85        89\n",
      "                                  backpropagation       0.88      0.93      0.91        89\n",
      "                              bacterial pneumonia       0.71      0.83      0.77        89\n",
      "                                  basilar papilla       0.91      0.92      0.91        64\n",
      "                                        beet pulp       0.96      0.94      0.95        90\n",
      "                                      bells palsy       0.80      0.85      0.83        89\n",
      "                                      bench press       0.95      0.94      0.95        89\n",
      "                                      benzapyrene       0.00      0.00      0.00        22\n",
      "                                     benzoapyrene       0.50      0.64      0.56        89\n",
      "                                     benzophenone       0.50      0.54      0.52        89\n",
      "                           bereitschaftspotential       1.00      0.05      0.09        22\n",
      "                                binding potential       0.69      0.64      0.66        89\n",
      "                                         biphenyl       0.60      0.69      0.64        89\n",
      "                                          bipolar       0.77      0.69      0.73        90\n",
      "                                    blastic phase       0.87      0.89      0.88        54\n",
      "                                     blood plasma       0.55      0.65      0.60        89\n",
      "                                      bodily pain       0.80      0.83      0.82        88\n",
      "                               bovine pericardium       0.87      0.92      0.90        89\n",
      "                               bowenoid papulosis       1.00      0.59      0.74        27\n",
      "                                  brachial plexus       0.87      0.80      0.83        89\n",
      "                                  brevipedicellus       0.00      0.00      0.00         6\n",
      "                            british pharmacopoeia       0.00      0.00      0.00        17\n",
      "                                  buffer capacity       0.49      0.76      0.60        90\n",
      "                                  buffering power       0.00      0.00      0.00        28\n",
      "                               bullous pemphigoid       0.76      0.90      0.82        89\n",
      "calcitonin generelated peptide cgrpimmunoreactive       0.00      0.00      0.00         7\n",
      "                              carbonic anhydrases       0.85      0.87      0.86        89\n",
      "                                 carotid arteries       0.71      0.64      0.67        89\n",
      "                                   catecholamines       0.71      0.49      0.58        89\n",
      "                              central compartment       0.51      0.71      0.59        90\n",
      "                                 circadian period       0.49      0.96      0.65        90\n",
      "                                     cold adapted       0.00      0.00      0.00        19\n",
      "                                 cold agglutinins       0.78      0.74      0.76        53\n",
      "                         constitutively activated       0.36      0.56      0.44        89\n",
      "                                  contrast agents       0.70      0.71      0.71        90\n",
      "                                coronary arteries       0.74      0.71      0.72        89\n",
      "                             culture supernatants       0.25      0.53      0.34        89\n",
      "                                       decay rate       0.58      0.69      0.63        89\n",
      "                   dementia of the alzheimer type       0.63      0.72      0.67        90\n",
      "                                  dosing interval       0.63      0.55      0.59        88\n",
      "                 doxorubicin and cyclophosphamide       0.83      0.78      0.80        55\n",
      "                                  edmonstonzagreb       0.81      0.93      0.87        14\n",
      "                                         efficacy       0.61      0.70      0.65        90\n",
      "                                      elimination       0.68      0.31      0.43        90\n",
      "                               epileptogenic zone       0.75      0.87      0.80        89\n",
      "                                      facial lobe       0.00      0.00      0.00         9\n",
      "                                     farmers lung       0.88      0.89      0.89        57\n",
      "                                      fetal liver       0.60      0.80      0.69        89\n",
      "                         filtration leukapheresis       0.00      0.00      0.00         8\n",
      "                               firefly luciferase       0.74      0.60      0.66        89\n",
      "                                      fluorescent       0.47      0.39      0.42        90\n",
      "                                fluorescent light       0.61      0.75      0.68        89\n",
      "                                        flutamide       0.77      0.82      0.79        89\n",
      "                                 foreign language       0.88      0.97      0.93        76\n",
      "                               freerunning period       0.00      0.00      0.00        48\n",
      "                                  friend leukemia       0.73      0.59      0.65        41\n",
      "                                     frontal lobe       0.70      0.79      0.74        89\n",
      "                                   fructoselysine       0.00      0.00      0.00        13\n",
      "                                       fulllength       0.57      0.64      0.60        89\n",
      "                               general population       0.68      0.69      0.69        90\n",
      "                                 general practice       0.81      0.80      0.80        88\n",
      "                         generalized polarization       0.00      0.00      0.00        21\n",
      "                         genetic haemochromatosis       0.00      0.00      0.00        14\n",
      "                          genetic hemochromatosis       0.50      0.04      0.08        24\n",
      "                              genetic programming       0.95      0.69      0.80        51\n",
      "                                       geniohyoid       0.87      0.92      0.89        36\n",
      "                                   genital herpes       0.84      0.93      0.88        89\n",
      "                         gestational hypertension       0.86      0.93      0.89        89\n",
      "                             gingival hyperplasia       0.65      0.93      0.77        81\n",
      "                                  gland of harder       0.00      0.00      0.00         3\n",
      "                                     glenohumeral       0.88      0.93      0.91        90\n",
      "                                  globus pallidus       0.90      0.79      0.84        89\n",
      "                                  glucose polymer       0.75      0.49      0.59        49\n",
      "                             glycolytic potential       0.00      0.00      0.00        26\n",
      "                                      glycophorin       0.66      0.74      0.70        89\n",
      "                              glycoside hydrolase       0.77      0.92      0.84        89\n",
      "                                      goodpasture       0.87      0.84      0.86        57\n",
      "                                         gossypol       0.66      0.93      0.78        89\n",
      "                                     grampositive       0.76      0.78      0.77        89\n",
      "                                     growth plate       0.76      0.79      0.78        90\n",
      "                                      guttapercha       0.91      0.97      0.94        89\n",
      "                                habitual abortion       0.82      0.42      0.56        33\n",
      "                                haemagglutinating       0.35      0.38      0.37        79\n",
      "                                haemagglutination       0.37      0.16      0.22        89\n",
      "                                    hairless gene       1.00      0.50      0.67        10\n",
      "                                    hbenzoapyrene       0.00      0.00      0.00        16\n",
      "                                   head activator       0.00      0.00      0.00        11\n",
      "                                         headache       0.78      0.85      0.82        89\n",
      "                                 health authority       0.89      0.90      0.89        89\n",
      "                                      hearing aid       0.95      0.88      0.91        89\n",
      "                                            heart       0.54      0.60      0.56        89\n",
      "                                       heart area       0.00      0.00      0.00         5\n",
      "                                 heat acclimation       0.89      0.87      0.88        89\n",
      "                                       height age       0.79      0.79      0.79        43\n",
      "                                 hemagglutinating       0.35      0.57      0.43        90\n",
      "                                 hemagglutination       0.24      0.35      0.28        89\n",
      "                        hemagglutination activity       0.00      0.00      0.00        42\n",
      "                           hemagglutination assay       0.00      0.00      0.00        42\n",
      "                       hemagglutinin glycoprotein       0.00      0.00      0.00         6\n",
      "                            hemagglutinin protein       0.00      0.00      0.00        33\n",
      "                                 hemolytic anemia       0.63      0.76      0.69        89\n",
      "                                heparinreleasable       0.76      0.78      0.77        41\n",
      "                                 hepatic arterial       0.47      0.84      0.61        90\n",
      "                                   hepatic artery       0.56      0.16      0.25        89\n",
      "                                hepatic resection       0.84      0.89      0.86        89\n",
      "                           hepatocellular adenoma       0.70      0.74      0.72        80\n",
      "                                    heptad repeat       0.68      0.90      0.78        89\n",
      "                              hippocampal atrophy       0.70      0.72      0.71        89\n",
      "                                histamine release       0.84      0.83      0.84        90\n",
      "                         homologous recombination       0.73      0.78      0.75        89\n",
      "                                 hormone receptor       0.68      0.72      0.70        89\n",
      "                                    human albumin       0.57      0.53      0.55        89\n",
      "                                 human fibroblast       0.74      0.67      0.71        89\n",
      "                                hybrid resistance       1.00      0.10      0.17        21\n",
      "                                hydropic abortion       0.00      0.00      0.00         5\n",
      "                             hydroxyapatitecoated       0.87      0.89      0.88        65\n",
      "                                 hyperandrogenism       0.60      0.88      0.72        89\n",
      "                          hypersensitive reaction       0.00      0.00      0.00        48\n",
      "                          hypersensitive response       0.54      0.91      0.68        89\n",
      "                        hyperstriatum accessorium       1.00      0.20      0.33        10\n",
      "                          hypothalamic amenorrhea       0.33      0.05      0.09        41\n",
      "                                implantation rate       0.79      0.85      0.82        89\n",
      "                                   inactive renin       0.94      0.94      0.94        89\n",
      "                                incomplete repair       0.00      0.00      0.00        16\n",
      "                                  inferior rectus       0.90      0.90      0.90        79\n",
      "                        influenza a hemagglutinin       0.00      0.00      0.00         4\n",
      "                    influenza virus hemagglutinin       0.37      0.83      0.51        71\n",
      "                            information retrieval       0.91      0.89      0.90        89\n",
      "                                intermediate risk       0.76      0.78      0.77        89\n",
      "                                internal rotation       0.85      0.79      0.82        89\n",
      "                             ionizing irradiation       0.45      0.19      0.27        88\n",
      "                               ionizing radiation       0.42      0.67      0.52        89\n",
      "                             ischaemiareperfusion       0.76      0.82      0.79        89\n",
      "              lines of chickens selected for high       0.00      0.00      0.00         2\n",
      "                      live attenuated coldadapted       0.00      0.00      0.00         5\n",
      "                                 mauthner neurons       0.00      0.00      0.00         7\n",
      "                                      meal number       1.00      0.18      0.30        17\n",
      "                                     median nerve       0.71      0.94      0.81        89\n",
      "                                 melanocytic nevi       0.89      0.94      0.92        89\n",
      "                                     mental nerve       0.97      0.51      0.67        57\n",
      "                       metabolism of benzoapyrene       0.00      0.00      0.00        21\n",
      "                                methyl nicotinate       0.00      0.00      0.00        15\n",
      "                   mice selectively bred for high       0.00      0.00      0.00         7\n",
      "                                     micronucleus       0.66      0.82      0.73        89\n",
      "                               min of reperfusion       0.71      0.90      0.80        89\n",
      "                 minor histocompatibility antigen       1.00      0.04      0.07        27\n",
      "                             myelin basic protein       0.62      0.67      0.65        89\n",
      "                                     nearinfrared       0.72      0.84      0.78        90\n",
      "             new zealand genetically hypertensive       0.00      0.00      0.00         7\n",
      "                 patients had alcoholic cirrhosis       0.00      0.00      0.00         3\n",
      "                                    period length       0.56      0.27      0.36        83\n",
      "                            platelet glycoprotein       0.67      0.85      0.75        89\n",
      "                                   pressure decay       0.64      0.45      0.53        31\n",
      "                              primary visual area       0.00      0.00      0.00        17\n",
      "                            primary visual cortex       0.47      0.63      0.54        89\n",
      "                           regression coefficient       0.67      0.54      0.60        89\n",
      "                                        rutosides       0.00      0.00      0.00         7\n",
      "                                  sensory neurons       0.52      0.71      0.60        89\n",
      "                                   sentinel nodes       0.81      0.74      0.77        90\n",
      "                                 skeletofusimotor       0.00      0.00      0.00         4\n",
      "                                spectral exponent       0.00      0.00      0.00         5\n",
      "                            sperm hyperactivation       0.00      0.00      0.00        13\n",
      "                                  stiffness index       0.67      0.86      0.76        88\n",
      "                              stiffness parameter       0.00      0.00      0.00        22\n",
      "                                   striate cortex       0.51      0.56      0.53        89\n",
      "           systolic and diastolic blood pressures       0.74      0.74      0.74        89\n",
      "                                  tangier disease       0.80      0.88      0.84        65\n",
      "                               tardive dyskinesia       0.69      0.89      0.78        89\n",
      "                                      tau protein       0.80      0.87      0.83        90\n",
      "                                       tdependent       0.75      0.58      0.66        89\n",
      "                              temporal difference       0.00      0.00      0.00        34\n",
      "                          thanatophoric dysplasia       0.91      0.79      0.85        38\n",
      "                                   thermodilution       0.79      0.93      0.86        89\n",
      "                              thiamine deficiency       0.80      0.84      0.82        89\n",
      "                               thiamine deficient       0.00      0.00      0.00        15\n",
      "                                    thoracic duct       0.84      0.89      0.86        89\n",
      "                          tibial dyschondroplasia       0.95      0.88      0.91        40\n",
      "                           time constant of decay       0.00      0.00      0.00        23\n",
      "           time constant of isovolumic relaxation       0.00      0.00      0.00         9\n",
      "                   time constant of lv relaxation       0.00      0.00      0.00         5\n",
      "                      time constant of relaxation       0.00      0.00      0.00         7\n",
      "                                       time delay       0.55      0.60      0.57        90\n",
      "                                   tissue doppler       0.82      0.89      0.85        89\n",
      "                                  total biopterin       0.00      0.00      0.00         9\n",
      "                                  total deviation       0.00      0.00      0.00         9\n",
      "                                       total dose       0.70      0.64      0.67        89\n",
      "                               tourettes disorder       0.00      0.00      0.00        25\n",
      "                                      transdermal       0.79      0.84      0.82        89\n",
      "                               travelers diarrhea       0.91      0.84      0.88        89\n",
      "                                      triple dose       0.00      0.00      0.00        16\n",
      "                                           trpasp       0.00      0.00      0.00         6\n",
      "                               true digestibility       0.71      0.59      0.65        37\n",
      "                       true protein digestibility       0.00      0.00      0.00         4\n",
      "                             tryptophan depletion       0.86      0.70      0.77        53\n",
      "                              typical development       0.75      0.70      0.72        89\n",
      "                             typically developing       0.71      0.81      0.76        89\n",
      "                                         vascular       0.33      0.31      0.32        89\n",
      "                                      vasopressor       0.49      0.63      0.55        89\n",
      "                           wallerian degeneration       0.63      0.72      0.67        89\n",
      "                                water deprivation       0.65      0.78      0.71        89\n",
      "                                     western diet       0.53      0.64      0.58        89\n",
      "                                 winter dysentery       0.00      0.00      0.00         9\n",
      "\n",
      "                                         accuracy                           0.70     17526\n",
      "                                        macro avg       0.56      0.55      0.54     17526\n",
      "                                     weighted avg       0.67      0.70      0.67     17526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# import torch\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "# import numpy as np\n",
    "\n",
    "def train_and_evaluate_abbreviation_model(train_texts, train_labels, valid_filtered, model_name=\"bert-base-uncased\", output_dir=\"./trained_model\", num_epochs=3, batch_size=16, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Trains a BERT-based model for abbreviation expansion and evaluates on a filtered validation dataset.\n",
    "    \n",
    "    Args:\n",
    "        train_texts (list): List of training texts.\n",
    "        train_labels (list): List of training labels.\n",
    "        valid_filtered (pd.DataFrame): Filtered validation dataset with 'TOKEN' and 'LABEL' columns.\n",
    "        model_name (str): Pretrained model name from Hugging Face.\n",
    "        output_dir (str): Directory to save the trained model.\n",
    "        num_epochs (int): Number of epochs for training.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics including precision, recall, F1-score, and accuracy.\n",
    "    \"\"\"\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize training data\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    # Encode labels for training\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "    \n",
    "    # Prepare validation texts and labels\n",
    "    valid_texts, valid_labels = prepare_text_and_labels(valid_filtered)\n",
    "    \n",
    "    # Tokenize validation data\n",
    "    valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n",
    "    valid_labels_encoded = label_encoder.transform(valid_labels)\n",
    "    \n",
    "    # Create PyTorch datasets\n",
    "    class AbbreviationDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels=None):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            if self.labels is not None:\n",
    "                item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "    train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n",
    "    valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n",
    "    \n",
    "    # Load pretrained model\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    valid_preds = trainer.predict(valid_dataset)\n",
    "    \n",
    "    # Extract true and predicted labels\n",
    "    predicted_labels = torch.argmax(torch.tensor(valid_preds.predictions), axis=1).numpy()\n",
    "    true_labels = valid_labels_encoded\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Confusion matrix and classification report\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    class_report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)\n",
    "    \n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "    \n",
    "    # Return metrics and other results\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"classification_report\": class_report\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "metrics = train_and_evaluate_abbreviation_model(\n",
    "    train_texts=train_texts,\n",
    "    train_labels=train_labels,\n",
    "    valid_filtered=valid_filtered,  # Validation data filtered for unseen labels\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    output_dir=\"./trained_model\",\n",
    "    num_epochs=3,\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if key == \"confusion_matrix\":\n",
    "        print(f\"{key}:\\n{value}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 965195,
     "sourceId": 1651976,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30066,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
