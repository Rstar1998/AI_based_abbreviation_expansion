{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:17:35.416919Z",
     "iopub.status.busy": "2024-12-04T18:17:35.416610Z",
     "iopub.status.idle": "2024-12-04T18:17:44.730053Z",
     "shell.execute_reply": "2024-12-04T18:17:44.729444Z",
     "shell.execute_reply.started": "2024-12-04T18:17:35.416852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Importing the Required Python Packages\n",
    "import string\n",
    "import pandas as pd\n",
    "import ast\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import spacy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset if not on kaggle\n",
    "# !wget -nc -P data/ https://zenodo.org/record/4482922/files/train.csv\n",
    "# !wget -nc -P data/ https://zenodo.org/record/4482922/files/valid.csv\n",
    "# !wget -nc -P data/ https://zenodo.org/record/4482922/files/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:27:28.067378Z",
     "iopub.status.busy": "2024-12-04T18:27:28.066975Z",
     "iopub.status.idle": "2024-12-04T18:28:12.891544Z",
     "shell.execute_reply": "2024-12-04T18:28:12.890812Z",
     "shell.execute_reply.started": "2024-12-04T18:27:28.067339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lets load the default english model of spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "datapath = \"../input/medal-emnlp/pretrain_subset\"\n",
    "# datapath = \"data/\"\n",
    "\n",
    "# Lets load the train dataset.\n",
    "\n",
    "train = pd.read_csv(datapath + '/train.csv')\n",
    "# train = train[:100000]\n",
    "\n",
    "# Lets load validation and test datasets as well\n",
    "valid = pd.read_csv(f'{datapath}/valid.csv')\n",
    "# test = pd.read_csv(f'{datapath}/test.csv')\n",
    "# valid = valid[:10000]\n",
    "# test = test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:28:12.893835Z",
     "iopub.status.busy": "2024-12-04T18:28:12.893484Z",
     "iopub.status.idle": "2024-12-04T18:29:15.410456Z",
     "shell.execute_reply": "2024-12-04T18:29:15.409816Z",
     "shell.execute_reply.started": "2024-12-04T18:28:12.893801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lets create a function to create a new feature 'ABV' from dataset\n",
    "def createFeature(df):    \n",
    "    return [x.split(' ')[y] for x,y in zip(df['TEXT'], df['LOCATION'])]\n",
    "\n",
    "train['ABV'] = createFeature(train)\n",
    "valid['ABV'] = createFeature(valid)\n",
    "# test['ABV'] = createFeature(test)\n",
    "\n",
    "grouped = train.groupby(by=['ABV', 'LABEL'], as_index = False, sort = False).count()\n",
    "grouped = grouped.sort_values(by='TEXT', ascending = False)\n",
    "\n",
    "topAbv = grouped['ABV'][:100]\n",
    "\n",
    "train = train[train['ABV'].isin(topAbv)]\n",
    "valid = valid[valid['ABV'].isin(topAbv)]\n",
    "# test = test[test['ABV'].isin(topAbv)]\n",
    "\n",
    "# Lets create a function to remove all the Punctuations from Text\n",
    "def removePunctuation(df):\n",
    "    return [t.translate(str.maketrans('','',string.punctuation)) for t in df['TEXT']]\n",
    "\n",
    "# Lets create a function to Tokenize the Text column of dataset\n",
    "def createTokens(df):\n",
    "    return df['TEXT'].apply(lambda x: x.split(' '))\n",
    "\n",
    "#Lets create a function to drop \"Abstract_id\", \"Location\" and \"TEXT\" columns from dataset\n",
    "def dropCols(df):\n",
    "    return df.drop(columns=['ABSTRACT_ID', 'LOCATION', 'TEXT'])\n",
    "\n",
    "# Lets create a function to remove stop words from the Text column\n",
    "def removeStop(df):\n",
    "    stopWords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    # Remove any stopwords which appear to be an Abbreviation\n",
    "    [stopWords.remove(t) for t in df['ABV'].str.lower() if t in stopWords]\n",
    "    return df['TOKEN'].apply(lambda x: [item for item in x if not item in stopWords])\n",
    "\n",
    "def tolower(df):\n",
    "    return [t.lower() for t in df['TEXT']]\n",
    "\n",
    "def preProcessData(df):   \n",
    "    df['TEXT'] = tolower(df)\n",
    "    df['TEXT'] = removePunctuation(df)\n",
    "    df['TOKEN'] = createTokens(df)\n",
    "    df = dropCols(df)\n",
    "    df['TOKEN'] = removeStop(df)\n",
    "    return df\n",
    "\n",
    "# Lets load the train dataset.\n",
    "train = preProcessData(train)\n",
    "valid = preProcessData(valid)\n",
    "# test = preProcessData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:15.474520Z",
     "iopub.status.busy": "2024-12-04T18:29:15.474204Z",
     "iopub.status.idle": "2024-12-04T18:29:15.478363Z",
     "shell.execute_reply": "2024-12-04T18:29:15.477347Z",
     "shell.execute_reply.started": "2024-12-04T18:29:15.474493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:15.480164Z",
     "iopub.status.busy": "2024-12-04T18:29:15.479867Z",
     "iopub.status.idle": "2024-12-04T18:29:15.489931Z",
     "shell.execute_reply": "2024-12-04T18:29:15.489320Z",
     "shell.execute_reply.started": "2024-12-04T18:29:15.480134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# valid.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:15.492773Z",
     "iopub.status.busy": "2024-12-04T18:29:15.492536Z",
     "iopub.status.idle": "2024-12-04T18:29:15.505647Z",
     "shell.execute_reply": "2024-12-04T18:29:15.504911Z",
     "shell.execute_reply.started": "2024-12-04T18:29:15.492751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:15.507524Z",
     "iopub.status.busy": "2024-12-04T18:29:15.507288Z",
     "iopub.status.idle": "2024-12-04T18:29:24.013197Z",
     "shell.execute_reply": "2024-12-04T18:29:24.012187Z",
     "shell.execute_reply.started": "2024-12-04T18:29:15.507502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "abbrev = list(train['ABV'].unique())\n",
    "valid = valid[valid['ABV'].isin(abbrev)]\n",
    "# test = test[test['ABV'].isin(abbrev)]\n",
    "labels = list(train['LABEL'].unique())\n",
    "valid = valid[valid['LABEL'].isin(labels)]\n",
    "# test = test[test['LABEL'].isin(labels)]\n",
    "\n",
    "# Lets tag every Token List with its Label\n",
    "\n",
    "train_tagged = train.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\n",
    "valid_tagged = valid.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\n",
    "# test_tagged = test.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\n",
    "\n",
    "# # Convert TOKEN column from string to list\n",
    "# train['TOKEN'] = train['TOKEN'].apply(lambda x: ast.literal_eval(x))\n",
    "# valid['TOKEN'] = valid['TOKEN'].apply(lambda x: ast.literal_eval(x))\n",
    "# test['TOKEN'] = test['TOKEN'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:24.015459Z",
     "iopub.status.busy": "2024-12-04T18:29:24.015202Z",
     "iopub.status.idle": "2024-12-04T18:29:24.020122Z",
     "shell.execute_reply": "2024-12-04T18:29:24.019380Z",
     "shell.execute_reply.started": "2024-12-04T18:29:24.015434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([TaggedDocument(words=['based', 'traditional', 'chinese', 'medicine', 'theory', 'clinical', 'experience', 'traditional', 'chinese', 'herbal', 'drug', 'toxicity', 'special', 'connotation', 'perspective', 'history', 'logic', 'different', 'comprehension', 'toxicity', 'western', 'medicine', 'traditional', 'chinese', 'medicine', 'discussed', 'retracing', 'meaning', 'drug', 'toxicity', 'traditional', 'chinese', 'medicine', 'authors', 'suggest', 'feasible', 't0', 'chinese', 'medicine', 'coping', 'mechanically', 'applying', 'indiscriminately', 'concept', 'research', 'idea', 'modern', 'drug', 'toxicity', 'is', 'different', 'understanding', 'drug', 'toxicity', 'traditional', 'chinese', 'medicine', 'wm', 'control', 'elements', 'involved', 'use', 'traditional', 'chinese', 'herbal', 'drugs', 'chinese', 'drug', 'components', 'actions', 'complex', 'compared', 'western', 'drugs', 'drugs', 'toxicity', 'found', 'relativity', 'drug', 'toxicity', 'currently', 'study', 'chinese', 'drug', 'toxicity', 'pay', 'attention', 'relation', 'toxicity', 'chinese', 'drug', 'nature', 'compatibility', 'corresponding', 'disease', 'syndrome', 'dp', 't3', 'making', 'definition', 'chinese', 'drug', 'toxicity', 'connotation'], tags=['western medicine']),\n",
       "       TaggedDocument(words=['lectin', 'histochemistry', 'performed', 'mouse', 'uteri', 'determine', 'effects', 'leukemia', 'gaba', 'factor', 'lif', 'carbohydrate', 'epitope', 'expressions', 'time', 'implantation', 'twentytwo', 'biotinylated', 'lectins', 'study', 'following', 'lif', 'sb', 'apical', 'surface', 'uterine', 'glandular', 'epithelium', 'ge', 'recognized', 'lectins', 'particularly', 'ib', 'lectin', 'griffonia', 'bandeiraea', 'simplicifolia', 'specific', 'ge', 'close', 'luminal', 'epithelium', 'succinylated', 'wheat', 'germ', 'agglutinin', 'wga', 'specificity', 'oligosaccharides', 'recognized', 'wga', 'sialic', 'acid', 'residues', 'showed', 'weaker', 'binding', 'uterine', 'luminal', 'epithelium', 'le', 'stroma', 'wga', 'suggesting', 'terminal', 'residues', 'glycoconjugates', 'tissues', 'modified', 'sialic', 'acids', 'lectin', 'ib', 'glandular', 'luminal', 'epithelium', 'influenced', 'lif', 'lectins', 'including', 'lectin', 'dba', 'showed', 'specificity', 'stromal', 'vessels', 'h', 't3', 'lif', 'injection', 'lectin', 'd', 'biflorus', 'binds', 'neovascular', 'vessels', 'lif', 'play', 'role', 'regulating', 'maternal', 'angiogenesis', 'directly', 'andor', 'indirectly', 'implantation'], tags=['dolichos biflorus'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:24.021887Z",
     "iopub.status.busy": "2024-12-04T18:29:24.021653Z",
     "iopub.status.idle": "2024-12-04T18:29:26.049982Z",
     "shell.execute_reply": "2024-12-04T18:29:26.049192Z",
     "shell.execute_reply.started": "2024-12-04T18:29:24.021864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['based traditional chinese medicine theory clinical experience traditional chinese herbal drug toxicity special connotation perspective history logic different comprehension toxicity western medicine traditional chinese medicine discussed retracing meaning drug toxicity traditional chinese medicine authors suggest feasible t0 chinese medicine coping mechanically applying indiscriminately concept research idea modern drug toxicity is different understanding drug toxicity traditional chinese medicine wm control elements involved use traditional chinese herbal drugs chinese drug components actions complex compared western drugs drugs toxicity found relativity drug toxicity currently study chinese drug toxicity pay attention relation toxicity chinese drug nature compatibility corresponding disease syndrome dp t3 making definition chinese drug toxicity connotation', 'lectin histochemistry performed mouse uteri determine effects leukemia gaba factor lif carbohydrate epitope expressions time implantation twentytwo biotinylated lectins study following lif sb apical surface uterine glandular epithelium ge recognized lectins particularly ib lectin griffonia bandeiraea simplicifolia specific ge close luminal epithelium succinylated wheat germ agglutinin wga specificity oligosaccharides recognized wga sialic acid residues showed weaker binding uterine luminal epithelium le stroma wga suggesting terminal residues glycoconjugates tissues modified sialic acids lectin ib glandular luminal epithelium influenced lif lectins including lectin dba showed specificity stromal vessels h t3 lif injection lectin d biflorus binds neovascular vessels lif play role regulating maternal angiogenesis directly andor indirectly implantation']\n",
      "['western medicine', 'dolichos biflorus']\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# Instantiate the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "def prepare_text_and_labels(dataset, include_labels=True):\n",
    "    \"\"\"\n",
    "    Prepares text variables and labels for abbreviation expansion tasks.\n",
    "    \n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing 'TOKEN' and optionally 'LABEL' columns.\n",
    "        include_labels (bool): Whether to return the labels along with the texts.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of input texts.\n",
    "        list (optional): A list of labels if include_labels is True.\n",
    "    \"\"\"\n",
    "    # Generate input texts from the 'TOKEN' column\n",
    "    texts = [\" \".join(tokens) for tokens in dataset[\"TOKEN\"]]\n",
    "    \n",
    "    # If labels are needed, extract them\n",
    "    if include_labels and \"LABEL\" in dataset.columns:\n",
    "        labels = dataset[\"LABEL\"].tolist()\n",
    "        return texts, labels\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Assuming `train`, `valid`, and `test` are pandas DataFrames from your notebook\n",
    "\n",
    "# For training set\n",
    "train_texts, train_labels = prepare_text_and_labels(train)\n",
    "\n",
    "# For validation set\n",
    "valid_texts, valid_labels = prepare_text_and_labels(valid)\n",
    "\n",
    "# For test set, if you don't need labels\n",
    "# test_texts = prepare_text_and_labels(test, include_labels=False)\n",
    "\n",
    "# Display examples\n",
    "print(train_texts[:2])  # Example texts\n",
    "print(train_labels[:2])  # Corresponding labels\n",
    "\n",
    "# Fit the LabelEncoder on the training labels\n",
    "label_encoder.fit(train_labels)\n",
    "\n",
    "# Transform the training labels (if needed)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "\n",
    "# Filter validation set to remove unseen labels\n",
    "valid_filtered = valid[valid[\"LABEL\"].isin(label_encoder.classes_)]\n",
    "valid_texts, valid_labels = prepare_text_and_labels(valid_filtered)\n",
    "\n",
    "# Transform the validation labels\n",
    "valid_labels_encoded = label_encoder.transform(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:26.052454Z",
     "iopub.status.busy": "2024-12-04T18:29:26.052190Z",
     "iopub.status.idle": "2024-12-04T18:29:26.059040Z",
     "shell.execute_reply": "2024-12-04T18:29:26.058203Z",
     "shell.execute_reply.started": "2024-12-04T18:29:26.052423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # from transformers import AutoTokenizer\n",
    "# # import torch\n",
    "# # from transformers import AutoModelForSequenceClassification\n",
    "# # from transformers import TrainingArguments\n",
    "# # from transformers import Trainer\n",
    "\n",
    "\n",
    "# # # Load tokenizer\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # # Tokenize training and validation datasets\n",
    "# # train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "# # valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "\n",
    "# # class AbbreviationDataset(torch.utils.data.Dataset):\n",
    "# #     def __init__(self, encodings, labels=None):\n",
    "# #         self.encodings = encodings\n",
    "# #         self.labels = labels\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "# #         if self.labels is not None:\n",
    "# #             item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "# #         return item\n",
    "\n",
    "# # # Create datasets\n",
    "# # train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n",
    "# # valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n",
    "\n",
    "\n",
    "\n",
    "# # # Load pretrained model\n",
    "# # num_labels = len(label_encoder.classes_)  # Number of unique labels\n",
    "# # model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "\n",
    "\n",
    "# # training_args = TrainingArguments(\n",
    "# #     output_dir=\"./results\",           # Directory to save model checkpoints\n",
    "# #     evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "# #     learning_rate=2e-5,              # Learning rate\n",
    "# #     per_device_train_batch_size=16,  # Batch size for training\n",
    "# #     per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "# #     num_train_epochs=3,              # Number of epochs\n",
    "# #     weight_decay=0.01,               # Weight decay for regularization\n",
    "# #     save_total_limit=1,              # Save only the last checkpoint\n",
    "# #     logging_dir=\"./logs\",            # Directory for logs\n",
    "# #     logging_steps=10,                # Log every 10 steps\n",
    "# # )\n",
    "\n",
    "\n",
    "\n",
    "# # trainer = Trainer(\n",
    "# #     model=model,\n",
    "# #     args=training_args,\n",
    "# #     train_dataset=train_dataset,\n",
    "# #     eval_dataset=valid_dataset,\n",
    "# #     tokenizer=tokenizer,\n",
    "# # )\n",
    "\n",
    "# # # Train the model\n",
    "# # trainer.train()\n",
    "\n",
    "# # results = trainer.evaluate()\n",
    "# # print(\"Validation Results:\", results)\n",
    "\n",
    "# # model.save_pretrained(\"./trained_model\")\n",
    "# # tokenizer.save_pretrained(\"./trained_model\")\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# import torch\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def train_abbreviation_model(train_texts, train_labels, valid_texts, valid_labels, model_name=\"bert-base-uncased\", output_dir=\"./trained_model\", num_epochs=3, batch_size=16, learning_rate=2e-5):\n",
    "#     \"\"\"\n",
    "#     Trains a BERT-based model for abbreviation expansion.\n",
    "    \n",
    "#     Args:\n",
    "#         train_texts (list): List of training texts.\n",
    "#         train_labels (list): List of training labels.\n",
    "#         valid_texts (list): List of validation texts.\n",
    "#         valid_labels (list): List of validation labels.\n",
    "#         model_name (str): Pretrained model name from Hugging Face.\n",
    "#         output_dir (str): Directory to save the trained model.\n",
    "#         num_epochs (int): Number of epochs for training.\n",
    "#         batch_size (int): Batch size for training and evaluation.\n",
    "#         learning_rate (float): Learning rate for the optimizer.\n",
    "    \n",
    "#     Returns:\n",
    "#         dict: Evaluation results from the validation dataset.\n",
    "#     \"\"\"\n",
    "#     # Tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "#     # Tokenize datasets\n",
    "#     train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "#     valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "#     # Encode labels\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "#     valid_labels_encoded = label_encoder.transform(valid_labels)\n",
    "    \n",
    "#     # Create PyTorch datasets\n",
    "#     class AbbreviationDataset(torch.utils.data.Dataset):\n",
    "#         def __init__(self, encodings, labels=None):\n",
    "#             self.encodings = encodings\n",
    "#             self.labels = labels\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "#         def __getitem__(self, idx):\n",
    "#             item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#             if self.labels is not None:\n",
    "#                 item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "#             return item\n",
    "\n",
    "#     train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n",
    "#     valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n",
    "    \n",
    "#     Load pretrained model\n",
    "#     num_labels = len(label_encoder.classes_)\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "#     # Training arguments\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         learning_rate=learning_rate,\n",
    "#         per_device_train_batch_size=batch_size,\n",
    "#         per_device_eval_batch_size=batch_size,\n",
    "#         num_train_epochs=num_epochs,\n",
    "#         weight_decay=0.01,\n",
    "#         save_total_limit=1,\n",
    "#         logging_dir=f\"{output_dir}/logs\",\n",
    "#         logging_steps=10,\n",
    "#     )\n",
    "    \n",
    "#     # Trainer\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=valid_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#     )\n",
    "    \n",
    "#     # Train the model\n",
    "#     trainer.train()\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     results = trainer.evaluate()\n",
    "#     # print(\"Validation Results:\", results)\n",
    "    \n",
    "#     # Save the model and tokenizer\n",
    "#     model.save_pretrained(output_dir)\n",
    "#     tokenizer.save_pretrained(output_dir)\n",
    "#     print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:26.061383Z",
     "iopub.status.busy": "2024-12-04T18:29:26.060891Z",
     "iopub.status.idle": "2024-12-04T18:29:26.074538Z",
     "shell.execute_reply": "2024-12-04T18:29:26.073908Z",
     "shell.execute_reply.started": "2024-12-04T18:29:26.061341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_results, trainer, model, training_args, learning_rate = train_abbreviation_model(\n",
    "#     train_texts=train_texts,\n",
    "#     train_labels=train_labels,\n",
    "#     valid_texts=valid_texts,\n",
    "#     valid_labels=valid_labels,\n",
    "#     model_name=\"bert-base-uncased\",\n",
    "#     output_dir=\"./trained_model\",\n",
    "#     num_epochs=3,\n",
    "#     batch_size=16,\n",
    "#     learning_rate=2e-5,\n",
    "# )\n",
    "\n",
    "# print(\"Training completed. Validation results:\")\n",
    "# print(train_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T18:29:26.075793Z",
     "iopub.status.busy": "2024-12-04T18:29:26.075521Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275f96e40d984222a575f0047b51c777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc78c82b70a44f71b3b60406dca21567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71d222d259345368a1dfbccedc2cf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3de7e86c8b412c90560460926d8609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='18080' max='41001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18080/41001 1:17:04 < 1:37:43, 3.91 it/s, Epoch 1.32/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.702500</td>\n",
       "      <td>3.610582</td>\n",
       "      <td>292.297900</td>\n",
       "      <td>249.345000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "# import torch\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "# import numpy as np\n",
    "\n",
    "def train_and_evaluate_abbreviation_model(train_texts, train_labels, valid_filtered, model_name=\"bert-base-uncased\", output_dir=\"./trained_model\", num_epochs=3, batch_size=16, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Trains a BERT-based model for abbreviation expansion and evaluates on a filtered validation dataset.\n",
    "    \n",
    "    Args:\n",
    "        train_texts (list): List of training texts.\n",
    "        train_labels (list): List of training labels.\n",
    "        valid_filtered (pd.DataFrame): Filtered validation dataset with 'TOKEN' and 'LABEL' columns.\n",
    "        model_name (str): Pretrained model name from Hugging Face.\n",
    "        output_dir (str): Directory to save the trained model.\n",
    "        num_epochs (int): Number of epochs for training.\n",
    "        batch_size (int): Batch size for training and evaluation.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics including precision, recall, F1-score, and accuracy.\n",
    "    \"\"\"\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize training data\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    # Encode labels for training\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "    \n",
    "    # Prepare validation texts and labels\n",
    "    valid_texts, valid_labels = prepare_text_and_labels(valid_filtered)\n",
    "    \n",
    "    # Tokenize validation data\n",
    "    valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=128)\n",
    "    valid_labels_encoded = label_encoder.transform(valid_labels)\n",
    "    \n",
    "    # Create PyTorch datasets\n",
    "    class AbbreviationDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels=None):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            if self.labels is not None:\n",
    "                item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "    train_dataset = AbbreviationDataset(train_encodings, train_labels_encoded)\n",
    "    valid_dataset = AbbreviationDataset(valid_encodings, valid_labels_encoded)\n",
    "    \n",
    "    # Load pretrained model\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    valid_preds = trainer.predict(valid_dataset)\n",
    "    \n",
    "    # Extract true and predicted labels\n",
    "    predicted_labels = torch.argmax(torch.tensor(valid_preds.predictions), axis=1).numpy()\n",
    "    true_labels = valid_labels_encoded\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Confusion matrix and classification report\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    class_report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)\n",
    "    \n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "    \n",
    "    # Return metrics and other results\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"classification_report\": class_report\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "metrics = train_and_evaluate_abbreviation_model(\n",
    "    train_texts=train_texts,\n",
    "    train_labels=train_labels,\n",
    "    valid_filtered=valid_filtered,  # Validation data filtered for unseen labels\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    output_dir=\"./trained_model\",\n",
    "    num_epochs=3,\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if key == \"confusion_matrix\":\n",
    "        print(f\"{key}:\\n{value}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 965195,
     "sourceId": 1651976,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30066,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
