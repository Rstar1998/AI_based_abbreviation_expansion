{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expansion(abbreviation):\n",
    "    abbreviation_lookup = {\n",
    "        \"R&D\": \"research and development\",\n",
    "        \"CPU\": \"central processing unit\",\n",
    "        \"RSVP\": \"répondez s'il vous plaît\",\n",
    "        \"VIP\": \"very important person\",\n",
    "        \"WHO\": \"World Health Organization\",\n",
    "        \"OKRs\": \"objectives and key results\",\n",
    "        \"GPS\": \"global positioning system\",\n",
    "        \"FAQ\": \"frequently asked questions\",\n",
    "        \"ATM\": \"automated teller machine\",\n",
    "        \"USB\": \"universal serial bus\",\n",
    "        \"NDA\": \"non-disclosure agreement\",\n",
    "        \"AI\": \"artificial intelligence\",\n",
    "        \"CCTV\": \"closed-circuit television\",\n",
    "        \"ETA\": \"estimated time of arrival\",\n",
    "        \"PDF\": \"portable document format\",\n",
    "        \"HR\": \"human resources\",\n",
    "        \"IPO\": \"initial public offering\",\n",
    "        \"RAM\": \"random-access memory\",\n",
    "        \"ROM\": \"read-only memory\",\n",
    "        \"CTO\": \"chief technology officer\",\n",
    "        \"ASAP\": \"as soon as possible\",\n",
    "        \"Wi-Fi\": \"wireless fidelity\",\n",
    "        \"SaaS\": \"software as a service\",\n",
    "        \"LAN\": \"local area network\",\n",
    "        \"IT\": \"information technology\",\n",
    "        \"MRI\": \"magnetic resonance imaging\",\n",
    "        \"ICU\": \"intensive care unit\",\n",
    "        \"FY\": \"fiscal year\",\n",
    "        \"BYOD\": \"bring your own device\",\n",
    "        \"POS\": \"point of sale\",\n",
    "        \"NGO\": \"non-governmental organization\",\n",
    "        \"HTTP\": \"hypertext transfer protocol\",\n",
    "        \"HTML\": \"hypertext markup language\",\n",
    "        \"URL\": \"uniform resource locator\",\n",
    "        \"QA\": \"quality assurance\",\n",
    "        \"SARS\": \"severe acute respiratory syndrome\",\n",
    "        \"CSV\": \"comma-separated values\",\n",
    "        \"SQL\": \"structured query language\",\n",
    "        \"API\": \"application programming interface\",\n",
    "        \"DNS\": \"domain name system\",\n",
    "        \"IP\": \"internet protocol\",\n",
    "        \"Bluetooth\": \"short-range wireless communication\"\n",
    "    }\n",
    "    return abbreviation_lookup.get(abbreviation, \"Unknown abbreviation\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict abbreviation expansion\n",
    "def predict_abbreviation_expansion(sentence: str, abbreviation: str, model_name: str = 'bert-base-uncased') -> str:\n",
    "    \"\"\"\n",
    "    Predict the expansion of an abbreviation in a given sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence containing the abbreviation to expand.\n",
    "        abbreviation (str): The abbreviation in the sentence to expand.\n",
    "        model_name (str): The name of the pretrained model to use (default: 'bert-base-uncased').\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted expansion for the abbreviation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Replace abbreviation with [MASK]\n",
    "        masked_sentence = sentence.replace(abbreviation, \"[MASK]\")\n",
    "        \n",
    "        # Tokenize Input\n",
    "        inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get Predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "        \n",
    "        # Find [MASK] token and its predicted token\n",
    "        masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "        predicted_token_id = predictions[0, masked_index].argmax(dim=1)\n",
    "        predicted_token = tokenizer.decode(predicted_token_id)\n",
    "        \n",
    "        return predicted_token.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error during prediction: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for fine-tuning\n",
    "class AbbreviationDataset(Dataset):\n",
    "    def __init__(self, sentences, abbreviations, expansions, tokenizer, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.abbreviations = abbreviations\n",
    "        self.expansions = expansions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx].replace(self.abbreviations[idx], \"[MASK]\")\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\", max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
    "        label = self.tokenizer(self.expansions[idx], return_tensors=\"pt\", max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
    "        inputs['labels'] = label['input_ids'].squeeze()\n",
    "        return {key: val.squeeze() for key, val in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine-tune the model\n",
    "def fine_tune_model(sentences, abbreviations, expansions, model_name='bert-base-uncased', epochs=3, batch_size=16, lr=5e-5):\n",
    "    \"\"\"\n",
    "    Fine-tune the BERT model for abbreviation expansion.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): List of input sentences.\n",
    "        abbreviations (list): List of abbreviations in the sentences.\n",
    "        expansions (list): List of correct expansions for the abbreviations.\n",
    "        model_name (str): Pretrained model name (default: 'bert-base-uncased').\n",
    "        epochs (int): Number of fine-tuning epochs (default: 3).\n",
    "        batch_size (int): Batch size for training (default: 16).\n",
    "        lr (float): Learning rate (default: 5e-5).\n",
    "\n",
    "    Returns:\n",
    "        AutoModelForMaskedLM: The fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    model.train()\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    dataset = AbbreviationDataset(sentences, abbreviations, expansions, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Fine-tuning loop\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['The R&D team delivered their findings to the board.', \"The CPU's performance was evaluated in the lab.\", 'The RSVP deadline for the event is tomorrow.', 'The VIP section was reserved for special guests.', 'The WHO released new guidelines on pandemic control.', 'The CEO presented the OKRs to the team.', 'The GPS system was accurate in navigating the route.', 'The FAQ section provides answers to common questions.', 'The ATM was out of cash this morning.', 'The USB drive contained all the project files.', 'The WHO is working on global health challenges.', 'The NDA prevents employees from sharing sensitive information.', 'The new AI model outperformed previous ones.', 'The CCTV footage was reviewed by security personnel.', 'The ETA for the delivery is tomorrow evening.', 'The PDF document was emailed to the client.', 'The HR team organized a training session.', 'The IPO of the company attracted many investors.', 'The RAM on the laptop was upgraded for better performance.', \"The ROM stores the device's firmware.\", 'The CTO discussed the technical roadmap for the next quarter.', 'The ASAP request required immediate attention.', 'The Wi-Fi network was slow due to high usage.', 'The SaaS business model is gaining popularity.', 'The LAN connection was stable during the meeting.', 'The IT team resolved the server issue quickly.', 'The MRI scan showed no abnormalities.', 'The ICU staff worked tirelessly to save lives.', 'The CEO signed off on the FY22 budget proposal.', 'The BYOD policy allows employees to use personal devices at work.', 'The POS terminal processed the payment successfully.', 'The NGO is working to provide education to underprivileged children.', 'The HTTP protocol is used for web communication.', 'The HTML file contained the structure of the webpage.', 'The URL was invalid and returned a 404 error.', 'The GPS directed us to the correct address.', 'The QA team tested the software for bugs.', 'The QA team found a bug in the software.', 'The SARS virus caused a global health emergency.', 'The NGO provided essential supplies to flood victims.', 'The CSV file was imported into the database.', 'The SQL query retrieved the required data from the database.', 'The API documentation was shared with the development team.', 'The DNS server resolved the domain name to an IP address.', 'The IP address was blocked for security reasons.', 'The Bluetooth connection was stable during the call.']\n",
      "Abbreviations: ['R&D', 'CPU', 'RSVP', 'VIP', 'WHO', 'OKRs', 'GPS', 'FAQ', 'ATM', 'USB', 'WHO', 'NDA', 'AI', 'CCTV', 'ETA', 'PDF', 'HR', 'IPO', 'RAM', 'ROM', 'CTO', 'ASAP', 'Wi-Fi', 'SaaS', 'LAN', 'IT', 'MRI', 'ICU', 'FY', 'BYOD', 'POS', 'NGO', 'HTTP', 'HTML', 'URL', 'GPS', 'QA', 'QA', 'SARS', 'NGO', 'CSV', 'SQL', 'API', 'DNS', 'IP', 'Bluetooth']\n",
      "Expansions: ['research and development', 'central processing unit', \"répondez s'il vous plaît\", 'very important person', 'World Health Organization', 'objectives and key results', 'global positioning system', 'frequently asked questions', 'automated teller machine', 'universal serial bus', 'World Health Organization', 'non-disclosure agreement', 'artificial intelligence', 'closed-circuit television', 'estimated time of arrival', 'portable document format', 'human resources', 'initial public offering', 'random-access memory', 'read-only memory', 'chief technology officer', 'as soon as possible', 'wireless fidelity', 'software as a service', 'local area network', 'information technology', 'magnetic resonance imaging', 'intensive care unit', 'fiscal year', 'bring your own device', 'point of sale', 'non-governmental organization', 'hypertext transfer protocol', 'hypertext markup language', 'uniform resource locator', 'global positioning system', 'quality assurance', 'quality assurance', 'severe acute respiratory syndrome', 'non-governmental organization', 'comma-separated values', 'structured query language', 'application programming interface', 'domain name system', 'internet protocol', 'short-range wireless communication']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/DJ/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 3/3 [00:14<00:00,  4.83s/it, loss=11.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 14.654485066731771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 3/3 [00:13<00:00,  4.45s/it, loss=8.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 9.288009961446127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 3/3 [00:13<00:00,  4.46s/it, loss=6.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 6.742023468017578\n",
      "Predicted Expansion for 'VIP': [PAD]\n",
      "Predicted Expansion for 'WHO': [PAD]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load the dataset\n",
    "    with open('abbreviation_dataset.json', 'r') as file:\n",
    "        dataset = json.load(file)\n",
    "\n",
    "    # Prepare sentences, abbreviations, and expansions\n",
    "    sentences = [item['sentence'] for item in dataset]\n",
    "    abbreviations = [item['abbreviation'] for item in dataset]\n",
    "    expansions = [item['expansion'] for item in dataset]\n",
    "\n",
    "    print(\"Sentences:\", sentences)\n",
    "    print(\"Abbreviations:\", abbreviations)\n",
    "    print(\"Expansions:\", expansions)\n",
    "\n",
    "    model = 'bert-base-uncased'\n",
    "\n",
    "    # Fine-tune the model\n",
    "    fine_tuned_model = fine_tune_model(sentences, abbreviations, expansions, model)\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    fine_tuned_model.save_pretrained(\"fine_tuned_abbreviation_model\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    tokenizer.save_pretrained(\"fine_tuned_abbreviation_model\")\n",
    "\n",
    "\n",
    "def predict_with_fine_tuned_model(sentence: str, abbreviation: str, model_path: str = \"fine_tuned_abbreviation_model\"):\n",
    "    \"\"\"\n",
    "    Use the fine-tuned model and tokenizer to predict abbreviation expansion.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The sentence containing the abbreviation.\n",
    "        abbreviation (str): The abbreviation to expand.\n",
    "        model_path (str): Path to the directory containing the fine-tuned model and tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted expansion for the abbreviation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the fine-tuned model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "        # Replace the abbreviation with [MASK]\n",
    "        masked_sentence = sentence.replace(abbreviation, \"[MASK]\")\n",
    "\n",
    "        # Tokenize Input\n",
    "        inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "\n",
    "        # Get Predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "        # Find the [MASK] token and its predicted token\n",
    "        masked_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "        predicted_token_id = predictions[0, masked_index].argmax(dim=1)\n",
    "        predicted_token = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "        return predicted_token.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error during prediction: {str(e)}\"\n",
    "\n",
    "\n",
    "# Example predictions\n",
    "test_sentence = \"The VIP section was reserved for special guests.\"\n",
    "test_abbreviation = \"VIP\"\n",
    "predicted_expansion = predict_with_fine_tuned_model(test_sentence, test_abbreviation)\n",
    "print(f\"Predicted Expansion for '{test_abbreviation}': {predicted_expansion}\")\n",
    "\n",
    "test_sentence_2 = \"The WHO released new guidelines on pandemic control.\"\n",
    "test_abbreviation_2 = \"WHO\"\n",
    "predicted_expansion_2 = predict_with_fine_tuned_model(test_sentence_2, test_abbreviation_2)\n",
    "print(f\"Predicted Expansion for '{test_abbreviation_2}': {predicted_expansion_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
